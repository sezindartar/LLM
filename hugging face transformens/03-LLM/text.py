import csv
csv.field_size_limit(10000000)

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
import numpy as np
import torch
import gradio as gr
from collections import Counter
import gc
import warnings
warnings.filterwarnings("ignore")

# ğŸ”¥ RTX 4050 OptimizasyonlarÄ±
def setup_rtx_4050():
    """RTX 4050 iÃ§in optimal CUDA ayarlarÄ±"""
    print("ğŸš€ RTX 4050 CUDA Optimizasyonu BaÅŸlatÄ±lÄ±yor...")
    
    # CUDA kontrol
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
        
        # CUDA optimizasyonlarÄ±
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # Memory management
        torch.cuda.empty_cache()
        
        return device
    else:
        print("âŒ CUDA bulunamadÄ±! CPU kullanÄ±lacak.")
        return torch.device("cpu")

# Setup
device = setup_rtx_4050()

# ğŸ¯ Memory-Efficient Dataset Loading
def load_medical_dataset():
    """Bellek verimli dataset yÃ¼kleme"""
    print("ğŸ“Š Dataset yÃ¼kleniyor...")
    
    try:
        dataset_dict = load_dataset("Intelligent-Internet/II-Medical-RL")
        print(f"âœ… Dataset yÃ¼klendi: {dataset_dict}")
        
        # Split kontrolÃ¼
        if 'test' in dataset_dict and 'train' in dataset_dict:
            train_dataset = dataset_dict['train']
            test_dataset = dataset_dict['test']
        else:
            full_dataset = dataset_dict['train']
            dataset_split = full_dataset.train_test_split(test_size=0.2, seed=42)
            train_dataset = dataset_split['train']
            test_dataset = dataset_split['test']
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶r
        label_counts = Counter(train_dataset['label'])
        print("ğŸ“ˆ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:", label_counts)
        
        # Etiket isimlerini belirle
        label_names = sorted(list(set(train_dataset["label"])))
        print("ğŸ·ï¸ Etiketler:", label_names)
        
        return train_dataset, test_dataset, label_names
        
    except Exception as e:
        print(f"âŒ Dataset yÃ¼kleme hatasÄ±: {e}")
        return None, None, None

# ğŸ¤– RTX 4050 Optimize Model Setup
def setup_optimized_model(label_names):
    """RTX 4050 iÃ§in optimize edilmiÅŸ model"""
    print("ğŸ¤– Model yÃ¼kleniyor...")
    
    # Daha kÃ¼Ã§Ã¼k model RTX 4050 iÃ§in
    model_name = "bert-base-uncased"  # KÃ¼Ã§Ã¼k model
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=len(label_names),
            torch_dtype=torch.float32,  # FP32 - stable training iÃ§in
        )
        
        # Model'i GPU'ya taÅŸÄ±
        model = model.to(device)
        
        # Model boyutunu gÃ¶ster
        param_count = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š Model parametreleri: {param_count:,}")
        print(f"ğŸ’¾ Model VRAM: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        return None, None

# ğŸ”„ Efficient Data Processing
def process_datasets(train_dataset, test_dataset, tokenizer, label_names):
    """Bellek verimli veri iÅŸleme"""
    print("ğŸ”„ Dataset iÅŸleniyor...")
    
    # Etiketleri sayÄ±sallaÅŸtÄ±r
    def encode_labels(example):
        example["labels"] = label_names.index(example["label"])
        return example
    
    # Tokenization function
    def tokenize(batch):
        return tokenizer(
            batch["question"], 
            padding="max_length", 
            truncation=True,
            max_length=256,  # RTX 4050 iÃ§in daha kÃ¼Ã§Ã¼k
            return_tensors="pt"
        )
    
    # Process datasets
    train_dataset = train_dataset.map(encode_labels, num_proc=4)
    test_dataset = test_dataset.map(encode_labels, num_proc=4)
    
    train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4)
    test_dataset = test_dataset.map(tokenize, batched=True, num_proc=4)
    
    # Set format
    train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
    test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
    
    print(f"âœ… Train samples: {len(train_dataset)}")
    print(f"âœ… Test samples: {len(test_dataset)}")
    
    return train_dataset, test_dataset

# ğŸ“Š Metrics Setup - sklearn-free version
def setup_metrics():
    """Metrik hesaplama - sklearn kullanmadan"""
    
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        
        # Manuel accuracy hesaplama
        accuracy = (predictions == labels).mean()
        
        # Manuel F1 hesaplama (macro)
        num_classes = len(np.unique(labels))
        f1_scores = []
        
        for class_id in range(num_classes):
            # Her sÄ±nÄ±f iÃ§in precision ve recall
            true_positives = ((predictions == class_id) & (labels == class_id)).sum()
            predicted_positives = (predictions == class_id).sum()
            actual_positives = (labels == class_id).sum()
            
            precision = true_positives / (predicted_positives + 1e-8)
            recall = true_positives / (actual_positives + 1e-8)
            
            f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
            f1_scores.append(f1)
        
        macro_f1 = np.mean(f1_scores)
        
        return {
            "accuracy": float(accuracy),
            "f1": float(macro_f1),
        }
    
    return compute_metrics

# ğŸ¯ RTX 4050 Optimized Training
def train_model(model, tokenizer, train_dataset, test_dataset, compute_metrics):
    """RTX 4050 iÃ§in optimize edilmiÅŸ eÄŸitim"""
    print("ğŸ¯ Model eÄŸitimi baÅŸlÄ±yor...")
    
    # RTX 4050 iÃ§in optimize edilmiÅŸ training arguments
    training_args = TrainingArguments(
        output_dir="./results",
        per_device_train_batch_size=2,    # Daha kÃ¼Ã§Ã¼k batch - RTX 4050 iÃ§in
        per_device_eval_batch_size=4,     # Eval iÃ§in biraz daha bÃ¼yÃ¼k
        gradient_accumulation_steps=4,    # Effective batch size = 2*4=8
        num_train_epochs=2,               # Daha az epoch, hÄ±zlÄ± test
        learning_rate=3e-5,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=200,
        save_strategy="steps", 
        save_steps=400,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        warmup_steps=200,
        fp16=False,                       # FP16 kapatÄ±ldÄ± - hata giderimi iÃ§in
        bf16=False,                       # BF16 de kapatÄ±ldÄ±
        dataloader_num_workers=0,         # CUDA iÃ§in
        remove_unused_columns=False,
        report_to="none",                 # Logging kapatÄ±ldÄ±
        seed=42,
        max_grad_norm=1.0,                # Gradient clipping
        save_total_limit=2,               # Disk alanÄ± tasarrufu
        prediction_loss_only=False,
        skip_memory_metrics=True,         # Memory metrics skip
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )
    
    # Memory monitoring
    def log_memory_usage():
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**2
            cached = torch.cuda.memory_reserved() / 1024**2
            print(f"ğŸ”‹ VRAM - KullanÄ±lan: {allocated:.1f}MB, Cached: {cached:.1f}MB")
    
    # EÄŸitim Ã¶ncesi memory
    log_memory_usage()
    
    # EÄŸitim
    try:
        print("ğŸš€ EÄŸitim baÅŸlÄ±yor (FP32 mode - stable)...")
        trainer.train()
        print("âœ… EÄŸitim tamamlandÄ±!")
        
        # Final evaluation
        eval_results = trainer.evaluate()
        print(f"ğŸ“Š Final Results:")
        print(f"  ğŸ“ˆ Accuracy: {eval_results['eval_accuracy']:.4f}")
        print(f"  ğŸ“ˆ F1 Score: {eval_results['eval_f1']:.4f}")
        
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("âŒ VRAM yetersiz! Daha kÃ¼Ã§Ã¼k batch size deneyin.")
            print("ğŸ’¡ Ã‡Ã¶zÃ¼m: batch_size=1, gradient_accumulation=8")
            return None
        else:
            print(f"âŒ EÄŸitim hatasÄ±: {e}")
            return None
    except Exception as e:
        print(f"âŒ Genel hata: {e}")
        return None
    
    # EÄŸitim sonrasÄ± memory
    log_memory_usage()
    
    return trainer

# ğŸ’¾ Model Saving
def save_model(model, tokenizer, save_path="./medical-bert-rtx4050"):
    """Model kaydetme"""
    print(f"ğŸ’¾ Model kaydediliyor: {save_path}")
    
    try:
        model.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)
        print("âœ… Model kaydedildi!")
        
    except Exception as e:
        print(f"âŒ Model kaydetme hatasÄ±: {e}")

# ğŸ¨ Gradio UI Setup
def create_inference_function(model, tokenizer, label_names, device):
    """Inference fonksiyonu"""
    
    # 4 sÄ±nÄ±f mapping
    answer_mapping = {
        "A": "ğŸ…° SeÃ§enek A",
        "B": "ğŸ…± SeÃ§enek B", 
        "C": "Â© SeÃ§enek C",
        "D": "ğŸ†” SeÃ§enek D"
    }
    
    model.eval()
    
    def predict_label(text):
        try:
            if len(text.strip().split()) < 3:
                return "âš  Soru Ã§ok kÄ±sa veya anlamlÄ± deÄŸil. LÃ¼tfen tÄ±bbi bir soru yazÄ±n."
            
            # Tokenize
            inputs = tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                padding=True,
                max_length=256
            )
            
            # GPU'ya taÅŸÄ±
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Inference
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                prediction = torch.argmax(logits, dim=1).item()
            
            predicted_label = label_names[prediction]
            turkish_answer = answer_mapping.get(predicted_label, f"â“ TanÄ±msÄ±z: {predicted_label}")
            
            # VRAM kullanÄ±mÄ±nÄ± gÃ¶ster
            vram_used = torch.cuda.memory_allocated() / 1024**2
            result = f"{turkish_answer}\n\nğŸ’¾ VRAM: {vram_used:.1f}MB"
            
            return result
            
        except Exception as e:
            return f"âŒ Hata oluÅŸtu: {str(e)}"
    
    return predict_label

# ğŸ® Gradio Interface
def launch_gradio_interface(predict_fn):
    """Gradio arayÃ¼zÃ¼"""
    
    interface = gr.Interface(
        fn=predict_fn,
        inputs=gr.Textbox(
            lines=3, 
            label="ğŸ¥ TÄ±bbi Soru (Ä°ngilizce)", 
            placeholder="Ã–rn: Are preterm twins at increased risk?"
        ),
        outputs=gr.Textbox(label="ğŸ¯ Tahmin Sonucu"),
        title="ğŸ¥ RTX 4050 TÄ±bbi Soru SÄ±nÄ±flandÄ±rma (BERT)",
        description="ğŸš€ RTX 4050 GPU ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ tÄ±bbi soru sÄ±nÄ±flandÄ±rma sistemi!",
        examples=[
            ["Are preterm twins at increased risk?"],
            ["Is vaccination safe during pregnancy?"],
            ["Can I take ibuprofen during breastfeeding?"],
            ["What are the symptoms of diabetes?"],
            ["How to treat high blood pressure?"],
        ],
        theme="default",
        allow_flagging="never"
    )
    
    return interface

# ğŸš€ Main Execution
def main():
    """Ana fonksiyon"""
    print("ğŸš€ RTX 4050 TÄ±bbi BERT Sistemi BaÅŸlatÄ±lÄ±yor...")
    print("=" * 60)
    
    # Dataset yÃ¼kleme
    train_dataset, test_dataset, label_names = load_medical_dataset()
    if train_dataset is None:
        print("âŒ Dataset yÃ¼klenemedi!")
        return
    
    # Model setup
    model, tokenizer = setup_optimized_model(label_names)
    if model is None:
        print("âŒ Model yÃ¼klenemedi!")
        return
    
    # Dataset processing
    train_dataset, test_dataset = process_datasets(train_dataset, test_dataset, tokenizer, label_names)
    
    # Metrics
    compute_metrics = setup_metrics()
    
    # Training
    trainer = train_model(model, tokenizer, train_dataset, test_dataset, compute_metrics)
    if trainer is None:
        print("âŒ EÄŸitim baÅŸarÄ±sÄ±z!")
        return
    
    # Model kaydetme
    save_model(model, tokenizer)
    
    # Gradio interface
    predict_fn = create_inference_function(model, tokenizer, label_names, device)
    interface = launch_gradio_interface(predict_fn)
    
    print("\nğŸ‰ RTX 4050 TÄ±bbi BERT Sistemi HazÄ±r!")
    print("ğŸŒ Gradio arayÃ¼zÃ¼ baÅŸlatÄ±lÄ±yor...")
    
    # Launch
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False
    )

if __name__ == "__main__":
    main()